{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4beb3759",
   "metadata": {},
   "source": [
    ":::{admonition} Download\n",
    ":class: important\n",
    "\n",
    "This notebook can be downloaded as **{nb-download}`introduction-stripped.ipynb`**. See the button at the top right to download as markdown or pdf.\n",
    "\n",
    ":::\n",
    "# Introduction, Text Removed\n",
    "\n",
    "This notebook has had all its explanatory text removed and has not been run.\n",
    " It is intended to be downloaded and run locally (or on the provided binder)\n",
    " while listening to the presenter's explanation. In order to see the fully\n",
    " rendered of this notebook, go [here](introduction.md)\n",
    "\n",
    ":::{admonition} Questions\n",
    ":class: important\n",
    "\n",
    "Throughout this notebook, there will be several questions that look like this. You are encouraged to stop and think about the question, to try and answer it yourself (perhaps looking at the hints that follow) before moving on and reading the answer below it.\n",
    "\n",
    ":::\n",
    "\n",
    "<img src=\"_static/models.png\">\n",
    "\n",
    "For the purposes of this notebook, we'll use some very simple convolutional models that are inspired by the processing done in the lateral geniculate nucleus (LGN) of the visual system[^models]. We're going to build up in complexity, starting with the Gaussian model at the top and gradually adding features[^notallmodels]. We'll describe the components of these models in more detail as we get to them, but briefly:\n",
    "\n",
    "[^models]: Most of these models were originally published in Berardino, A., Laparra, V., J Ball\\'e, & Simoncelli, E. P. (2017). Eigen-distortions of hierarchical representations. In Adv. Neural Information Processing Systems (NIPS*17), from which the figure is modified.\n",
    "\n",
    "[^notallmodels]: Note that the Berardino et. al, 2017 paper includes more models than described here. We're not examining all of them for time's sake, but you can check out the rest of the models described in the Berardino paper, they're all included in plenoptic under the [plenoptic.simulate.FrontEnd](https://docs.plenoptic.org/docs/branch/main/api/plenoptic.simulate.models.html#module-plenoptic.simulate.models.frontend) module!\n",
    "\n",
    "- `Gaussian`: the model just convolves a Gaussian with an image, so that the model's representation is simply a blurry version of the image.\n",
    "- `CenterSurround`: the model convolves a difference-of-Gaussian filter with the image, so that model's representation is bandpass, caring mainly about frequencies that are neither too high or too low.\n",
    "- `LuminanceGainControl`: the model rectifies and normalizes the linear component of the response using a local measure of luminance, so that the response is invariant to local changes in luminance.\n",
    "\n",
    "## Plenoptic basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plenoptic as po\n",
    "import torch\n",
    "import pyrtools as pt\n",
    "import matplotlib.pyplot as plt\n",
    "# so that relative sizes of axes created by po.imshow and others look right\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "plt.rcParams['animation.html'] = 'html5'\n",
    "# use single-threaded ffmpeg for animation writer\n",
    "plt.rcParams['animation.writer'] = 'ffmpeg'\n",
    "plt.rcParams['animation.ffmpeg_args'] = ['-threads', '1']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(\"Running on GPU!\")\n",
    "else:\n",
    "    print(\"Running on CPU!\")\n",
    "# for reprodicibility\n",
    "po.tools.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = po.data.einstein().to(DEVICE)\n",
    "fig = po.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb739b",
   "metadata": {},
   "source": [
    "Set up the Guassian model. Models in plenoptic must:\n",
    "- Inherit `torch.nn.Module`.\n",
    "- Accept tensors as input and return tensors as output.\n",
    "- Have `forward` and `__init__` methods.\n",
    "- Have all model parameter gradients removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e456007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a convenience function for creating a simple Gaussian kernel\n",
    "from plenoptic.simulate.canonical_computations.filters import circular_gaussian2d\n",
    "\n",
    "# Simple Gaussian convolutional model\n",
    "class Gaussian(torch.nn.Module):\n",
    "    # in __init__, we create the object, initializing the convolutional weights and nonlinearity\n",
    "    def __init__(self, kernel_size, std_dev=3):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = torch.nn.Conv2d(1, 1, kernel_size=kernel_size, padding=(0, 0), bias=False)\n",
    "        self.conv.weight.data[0, 0] = circular_gaussian2d(kernel_size, float(std_dev))\n",
    "        \n",
    "    # the forward pass of the model defines how to get from an image to the representation\n",
    "    def forward(self, x):\n",
    "        x = po.tools.conv.same_padding(x, self.kernel_size, pad_mode='circular')\n",
    "        return self.conv(x)\n",
    "\n",
    "# we pick this particular number to match the models found in the Berardino paper\n",
    "model = Gaussian((31, 31)).to(DEVICE)\n",
    "rep = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "print(rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.tools.remove_grad(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6df53",
   "metadata": {},
   "source": [
    "- The Gaussian model output is a blurred version of the input.\n",
    "- This is because the model is preserving the low frequencies,  discarding the high frequencies (i.e., it's a lowpass filter).\n",
    "- Thus, this model is completely insensitive to high frequencies -- information there is invisible to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = po.imshow(torch.cat([img, rep]), title=['Original image', 'Model output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fd50a",
   "metadata": {},
   "source": [
    "## Examining model invariances with metamers\n",
    "\n",
    "- Initialize the `Metamer` object and synthesize a model metamer.\n",
    "- View the synthesis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamer = po.synthesize.Metamer(img, model)\n",
    "\n",
    "matched_im = metamer.synthesize(store_progress=True, max_iter=20)\n",
    "# if we call synthesize again, we resume where we left off\n",
    "matched_im = metamer.synthesize(store_progress=True, max_iter=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec42462",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.synthesize.metamer.plot_loss(metamer);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376095a",
   "metadata": {},
   "source": [
    ":::{important} \n",
    "This next cell will take a while to run --- making animations in matplotlib is a bit of a slow process.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.synthesize.metamer.animate(metamer, included_plots=['display_metamer', 'plot_loss'], figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc21cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = po.imshow([img, rep, metamer.metamer, model(metamer.metamer)], \n",
    "                col_wrap=2, vrange='auto1',\n",
    "                title=['Original image', 'Model representation\\nof original image',\n",
    "                       'Synthesized metamer', 'Model representation\\nof synthesized metamer']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede73526",
   "metadata": {},
   "source": [
    ":::{admonition} Question\n",
    ":class: important\n",
    "\n",
    "Why does the model metamer look \"staticky\"?\n",
    ":::\n",
    ":::{admonition} Hint\n",
    ":class: hint dropdown\n",
    "\n",
    "Model metamers help us examine the model's nullspace, its invariances. A Gaussian is a lowpass filter, so what information is it insensitive to?\n",
    ":::\n",
    "\n",
    "- Synthesize more model metamers, from different starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "curie = po.data.curie().to(DEVICE)\n",
    "# pyrtools, imported as pt, has a convenience function for generating samples of white noise, but then we still \n",
    "# need to do some annoying things to get it ready for plenoptic\n",
    "pink = torch.from_numpy(pt.synthetic_images.pink_noise((256, 256))).unsqueeze(0).unsqueeze(0)\n",
    "pink = po.tools.rescale(pink).to(torch.float32).to(DEVICE)\n",
    "po.imshow([curie, pink]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamer_curie = po.synthesize.Metamer(img, model)\n",
    "metamer_curie.setup(initial_image=curie)\n",
    "metamer_pink = po.synthesize.Metamer(img, model) \n",
    "metamer_pink.setup(initial_image=pink)\n",
    "\n",
    "# we increase the length of time we run synthesis and decrease the\n",
    "# stop_criterion, which determines when we think loss has converged\n",
    "# for stopping synthesis early.\n",
    "metamer_curie.synthesize(max_iter=500,  stop_criterion=1e-7)\n",
    "metamer_pink.synthesize(max_iter=500,  stop_criterion=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.synthesize.metamer.plot_loss(metamer_curie)\n",
    "po.synthesize.metamer.plot_loss(metamer_pink);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf16b0",
   "metadata": {},
   "source": [
    "In the following plot:\n",
    "- the first row shows our target Einstein image and its model representation, as we saw before.\n",
    "- the new three rows show our model metamers resulting from three different starting points.\n",
    "- in each, the first column shows the starting point of our metamer synthesis, the middle shows the resulting model metamer, and the third shows the model representation.\n",
    "\n",
    "We can see that the model representation is the same for all four images, but the images themselves look very different. Because the model is completely invariant to high frequencies, the high frequencies present in the initial image are not affected by the synthesis procedure and thus are still present in the model metamer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47990ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = po.imshow([torch.ones_like(img), img, rep,\n",
    "                 metamer.saved_metamer[0], metamer.metamer, model(metamer.metamer),\n",
    "                 pink, metamer_pink.metamer, model(metamer_pink.metamer),\n",
    "                 curie, metamer_curie.metamer, model(metamer_curie.metamer)],\n",
    "                col_wrap=3, vrange='auto1',\n",
    "                title=['', 'Original image', 'Model representation\\nof original image']+\n",
    "                      3*['Initial image', 'Synthesized metamer', 'Model representation\\nof synthesized metamer']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8411042",
   "metadata": {},
   "source": [
    "## Examining model sensitivies to eigendistortions\n",
    "\n",
    "- While metamers allow us to examine model invariances, eigendistortions allow us to also examine model sensitivities.\n",
    "- Eigendistortions are distortions that the model thinks are the most and least noticeable.\n",
    "- They can be visualized on their own or on top of the reference image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = po.synthesize.Eigendistortion(img, model)\n",
    "eig.synthesize();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow(eig.eigendistortions, title=['Maximum eigendistortion', \n",
    "                                       'Minimum eigendistortion']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948be5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow(img + 3*eig.eigendistortions, title=['Maximum eigendistortion', \n",
    "                                               'Minimum eigendistortion']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46af253",
   "metadata": {},
   "source": [
    "## A more complex model\n",
    "\n",
    "- The `CenterSurround` model has bandpass sensitivity, as opposed to the `Gaussian`'s lowpass.\n",
    "- Thus, it is still insensitive to the highest frequencies, but it is less sensitive to the low frequencies the Gaussian prefers, with its peak sensitivity lying in a middling range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values come from Berardino et al., 2017.\n",
    "center_surround = po.simulate.CenterSurround((31, 31), center_std=1.962, surround_std=4.235,\n",
    "                                             pad_mode='circular').to(DEVICE)\n",
    "po.tools.remove_grad(center_surround)\n",
    "center_surround.eval()\n",
    "center_surround(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow([img, center_surround(img)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c73f59",
   "metadata": {},
   "source": [
    "- We can synthesize all three model metamers at once by taking advantage of multi-batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ac836",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise =  po.tools.rescale(torch.rand_like(img), a=0, b=1).to(DEVICE)\n",
    "init_img = torch.cat([white_noise, pink, curie], dim=0)\n",
    "# metamer does a 1-to-1 matching between initial and target images,\n",
    "# so we need to repeat the target image on the batch dimension\n",
    "cs_metamer = po.synthesize.Metamer(img.repeat(3, 1, 1, 1), center_surround)\n",
    "cs_metamer.setup(initial_image=init_img)\n",
    "cs_metamer.synthesize(1000, stop_criterion=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f444c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this requires a little reorganization of the tensors:\n",
    "to_plot = [torch.cat([torch.ones_like(img), img, center_surround(img)])]\n",
    "for i, j in zip(init_img, cs_metamer.metamer):\n",
    "    to_plot.append(torch.stack([i, j, center_surround(j)]))\n",
    "to_plot = torch.cat(to_plot)\n",
    "fig = po.imshow(to_plot, col_wrap=3, \n",
    "                title=['', 'Original image', 'Model representation\\nof original image']+\n",
    "                       3*['Initial image', 'Synthesized metamer', 'Model representation\\nof synthesized metamer']);\n",
    "# change the color scale of the images so that the first two columns go from 0 to 1 \n",
    "# and the last one is consistent\n",
    "for ax in fig.axes:\n",
    "    if 'representation' in ax.get_title():\n",
    "        clim = (to_plot[2::3].min(), to_plot[2::3].max())\n",
    "    else:\n",
    "        clim = (0, 1)\n",
    "    ax.images[0].set_clim(*clim)\n",
    "    title = ax.get_title().split('\\n')\n",
    "    title[-2] = f\" range: [{clim[0]:.01e}, {clim[1]:.01e}]\" \n",
    "    ax.set_title('\\n'.join(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebcb9b",
   "metadata": {},
   "source": [
    ":::{admonition} Question\n",
    ":class: important\n",
    "\n",
    "How do these model metamers differ from the Gaussian ones? How does that relate to what we know about the model's sensitivities and invariances?\n",
    ":::\n",
    "\n",
    "- By examining the eigendistortions, we can see more clearly that the model's preferred frequency has shifted higher, while the minimal eigendistortion still looks fairly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_eig = po.synthesize.Eigendistortion(img, center_surround)\n",
    "cs_eig.synthesize();\n",
    "po.imshow(cs_eig.eigendistortions, title=['Maximum eigendistortion', \n",
    "                                          'Minimum eigendistortion']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101312e",
   "metadata": {},
   "source": [
    "## Adding some nonlinear features to the mix\n",
    "\n",
    "- The `LuminanceGainControl` model adds a nonlinearity, gain control. This makes the model harder to reason than the first two models.\n",
    "\n",
    "- This is a computation that we think is present throughout much of the early visual system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = po.simulate.LuminanceGainControl((31, 31), pad_mode=\"circular\").to(DEVICE)\n",
    "params_dict = {'luminance_scalar': 14.95, 'luminance.std': 4.235, \n",
    "               'center_surround.center_std': 1.962, 'center_surround.surround_std': 4.235,\n",
    "               'center_surround.amplitude_ratio': 1.25}\n",
    "lg.load_state_dict({k: torch.as_tensor([v]) for k, v in params_dict.items()})\n",
    "po.tools.remove_grad(lg)\n",
    "lg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d203a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow([lg(img), lg(2*img)], vrange='auto1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_metamer = po.synthesize.Metamer(img.repeat(3, 1, 1, 1), lg)\n",
    "lg_metamer.setup(initial_image=init_img, optimizer_kwargs={\"lr\": .007})\n",
    "lg_metamer.synthesize(3500, stop_criterion=1e-11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5e179",
   "metadata": {},
   "source": [
    "- The model metamers here look fairly similar to those of the `CenterSurround` model, though you can see these are more \"gray\", because this model is even less sensitive to the local luminance than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc952c8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# this requires a little reorganization of the tensors:\n",
    "to_plot = [torch.cat([torch.ones_like(img), img, center_surround(img)])]\n",
    "for i, j in zip(init_img, lg_metamer.metamer):\n",
    "    to_plot.append(torch.stack([i, j, center_surround(j)]))\n",
    "to_plot = torch.cat(to_plot)\n",
    "fig = po.imshow(to_plot, col_wrap=3, \n",
    "                title=['', 'Original image', 'Model representation\\nof original image']+\n",
    "                       3*['Initial image', 'Synthesized metamer', 'Model representation\\nof synthesized metamer']);\n",
    "# change the color scale of the images so that the first two columns go from 0 to 1 \n",
    "# and the last one is consistent\n",
    "for ax in fig.axes:\n",
    "    if 'representation' in ax.get_title():\n",
    "        clim = (to_plot[2::3].min(), to_plot[2::3].max())\n",
    "    else:\n",
    "        clim = (0, 1)\n",
    "    ax.images[0].set_clim(*clim)\n",
    "    title = ax.get_title().split('\\n')\n",
    "    title[-2] = f\" range: [{clim[0]:.01e}, {clim[1]:.01e}]\" \n",
    "    ax.set_title('\\n'.join(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_eig = po.synthesize.Eigendistortion(img, lg)\n",
    "lg_eig.synthesize();\n",
    "po.imshow(lg_eig.eigendistortions, title=['Maximum eigendistortion', \n",
    "                                          'Minimum eigendistortion']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cfdf0",
   "metadata": {},
   "source": [
    ":::{admonition} Question\n",
    ":class: important\n",
    "\n",
    "How do these eigendistortions compare to that of the `CenterSurround` model? Why do they, especially the maximum eigendistortion, look more distinct from those of the `CenterSurround` model than the metamers do?\n",
    ":::\n",
    ":::{admonition} Hint\n",
    ":class: hint dropdown\n",
    "\n",
    "The maximum eigendistortion emphasizes what the model is *most* sensitive to (whereas metamers focus on model invariances), so what about the `LinearGainControl` model's nonlinearities would cause this change?\n",
    ":::\n",
    "\n",
    "- Gain control makes this model adaptive, and thus the location of the eigendistortion matters, which was not true of our previous, linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_curie_eig = po.synthesize.Eigendistortion(curie, lg)\n",
    "lg_curie_eig.synthesize();\n",
    "po.imshow(lg_curie_eig.eigendistortions, title=['LG Maximum eigendistortion\\n (on Curie)', \n",
    "                                                'LG Minimum eigendistortion\\n (on Curie)']);\n",
    "cs_curie_eig = po.synthesize.Eigendistortion(curie, center_surround)\n",
    "cs_curie_eig.synthesize();\n",
    "po.imshow(cs_curie_eig.eigendistortions, title=['CenterSurround Maximum \\neigendistortion (on Curie)', \n",
    "                                                'CenterSurround Minimum \\neigendistortion (on Curie)']);\n",
    "po.imshow(cs_eig.eigendistortions, title=['CenterSurround Maximum \\neigendistortion (on Einstein)', \n",
    "                                          'CenterSurround Minimum \\neigendistortion (on Einstein)']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the [:1] is a trick to get only the first element while still being a 4d\n",
    "# tensor\n",
    "po.imshow([img+3*lg_eig.eigendistortions[:1],\n",
    "           curie+3*lg_curie_eig.eigendistortions[:1]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c074698",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow(img+3*lg_eig.eigendistortions[:1].roll(128, -1))\n",
    "print(f\"Max LG eigendistortion: {po.tools.l2_norm(lg(img), lg(img+lg_eig.eigendistortions[:1]))}\")\n",
    "print(f\"Shifted max LG eigendistortion: {po.tools.l2_norm(lg(img), lg(img+lg_eig.eigendistortions[:1].roll(128, -1)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c8f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max CenterSurround eigendistortion: {po.tools.l2_norm(center_surround(img), center_surround(img+cs_eig.eigendistortions[:1]))}\")\n",
    "print(f\"Shifted max CenterSurround eigendistortion: {po.tools.l2_norm(center_surround(img), center_surround(img+cs_eig.eigendistortions[:1].roll(128, -1)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8849fd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we saw the basics of using `plenoptic` to investigate the sensitivities and invariances of some simple convolutional models, and reasoned through how the model metamers and eigendistortions we saw enable us to understand how these models process images.\n",
    "\n",
    "`plenoptic` includes a variety of models and model components in the [plenoptic.simulate](https://docs.plenoptic.org/docs/branch/main/api/plenoptic.simulate.html) module, and you can (and should!) use the synthesis methods with your own models. Our documentation also has [examples](https://docs.plenoptic.org/docs/branch/main/tutorials/applications/Demo_Eigendistortion.html) showing how to use models from [torchvision](https://pytorch.org/vision/stable/index.html) (which contains a variety of pretrained neural network models) with plenoptic (we'll be releasing a simpler interface for torchvision and other pytorch model zoos this summer -- ask me if you're interested!). In order to use your own models with plenoptic, check the [documentation](https://docs.plenoptic.org/docs/branch/main/models.html) for the specific requirements, and use the [`validate_model`](https://docs.plenoptic.org/docs/branch/main/api/plenoptic.tools.html#plenoptic.tools.validate.validate_model) function to check compatibility. If you have issues or want feedback, we're happy to help --- just post on the [Github discussions page](https://github.com/plenoptic-org/plenoptic/discussions)!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.2"
   }
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   46,
   68,
   71,
   79,
   101,
   105,
   108,
   114,
   116,
   122,
   129,
   131,
   137,
   140,
   145,
   159,
   167,
   179,
   182,
   191,
   199,
   206,
   210,
   214,
   217,
   223,
   231,
   233,
   237,
   246,
   268,
   277,
   282,
   289,
   298,
   301,
   305,
   309,
   332,
   337,
   351,
   363,
   369,
   374,
   377
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}